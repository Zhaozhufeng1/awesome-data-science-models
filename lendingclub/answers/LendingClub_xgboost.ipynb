{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to train your model on AI Platform with HP tuning.\n",
    "Using HP Tuning for training can be done in a few steps:\n",
    "1. Create your python model file\n",
    "    1. Add argument parsing for the hyperparameter values. (These values are chosen for you in this notebook)\n",
    "    1. Add code to download your data from [Google Cloud Storage](https://cloud.google.com/storage) so that AI Platform can use it\n",
    "    1. Add code to track the performance of your hyperparameter values.\n",
    "    1. Add code to export and save the model to [Google Cloud Storage](https://cloud.google.com/storage) once AI Platform finishes training the model\n",
    "1. Prepare a package\n",
    "1. Submit the training job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "Before you jump in, let’s cover some of the different tools you’ll be using to get HP tuning up and running on AI Platform. \n",
    "\n",
    "[Google Cloud Platform](https://cloud.google.com/) lets you build and host applications and websites, store data, and analyze data on Google's scalable infrastructure.\n",
    "\n",
    "[AI Platform](https://cloud.google.com/ml-engine/) is a managed service that enables you to easily build machine learning models that work on any type of data, of any size.\n",
    "\n",
    "[Google Cloud Storage](https://cloud.google.com/storage/) (GCS) is a unified object storage for developers and enterprises, from live data serving to data analytics/ML to data archiving.\n",
    "\n",
    "[Cloud SDK](https://cloud.google.com/sdk/) is a command line tool which allows you to interact with Google Cloud products. In order to run this notebook, make sure that Cloud SDK is [installed](https://cloud.google.com/sdk/downloads) in the same environment as your Jupyter kernel.\n",
    "\n",
    "[Overview of Hyperparameter Tuning](https://cloud.google.com/ml-engine/docs/tensorflow/hyperparameter-tuning-overview) - Hyperparameter tuning takes advantage of the processing infrastructure of Google Cloud Platform to test different hyperparameter configurations when training your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: Setup\n",
    "* [Create a project on GCP](https://cloud.google.com/resource-manager/docs/creating-managing-projects)\n",
    "* [Create a Google Cloud Storage Bucket](https://cloud.google.com/storage/docs/quickstart-console)\n",
    "* [Enable AI Platform Training and Prediction and Compute Engine APIs](https://console.cloud.google.com/flows/enableapi?apiid=ml.googleapis.com,compute_component&_ga=2.217405014.1312742076.1516128282-1417583630.1516128282)\n",
    "* [Install Cloud SDK](https://cloud.google.com/sdk/downloads)\n",
    "* [Install XGBoost](https://xgboost.readthedocs.io/en/latest/build.html) [Optional: used if running locally]\n",
    "* [Install pandas](https://pandas.pydata.org/pandas-docs/stable/install.html) [Optional: used if running locally]\n",
    "* [Install cloudml-hypertune](https://pypi.org/project/cloudml-hypertune/) [Optional: used if running locally]\n",
    "\n",
    "These variables will be needed for the following steps.\n",
    "* `TRAINER_PACKAGE_PATH <./hp_tuning>` - A packaged training application that will be staged in a Google Cloud Storage location. The model file created below is placed inside this package path.\n",
    "* `MAIN_TRAINER_MODULE <hp_tuning.train>` - Tells AI Platform which file to execute. This is formatted as follows <folder_name.python_file_name>\n",
    "* `JOB_DIR <gs://$BUCKET_ID/xgboost_learn_job_dir>` - The path to a Google Cloud Storage location to use for job output.\n",
    "* `RUNTIME_VERSION <1.9>` - The version of AI Platform to use for the job. If you don't specify a runtime version, the training service uses the default AI Platform runtime version 1.0. [See the list of runtime versions for more information](https://cloud.google.com/ml-engine/docs/tensorflow/runtime-version-list).\n",
    "* `PYTHON_VERSION <3.5>` - The Python version to use for the job. Python 3.5 is available with runtime version 1.4 or greater. If you don't specify a Python version, the training service uses Python 2.7.\n",
    "* `HPTUNING_CONFIG <hptuning_config.yaml>` - Path to the job configuration file.\n",
    "\n",
    "** Replace: **\n",
    "* `PROJECT_ID <YOUR_PROJECT_ID>` - with your project's id. Use the PROJECT_ID that matches your Google Cloud Platform project.\n",
    "* `BUCKET_ID <YOUR_BUCKET_ID>` - with the bucket id you created above.\n",
    "* `JOB_DIR <gs://YOUR_BUCKET_ID/xgboost_job_dir>` - with the bucket id you created above.\n",
    "* `REGION <REGION>` - select a region from [here](https://cloud.google.com/ml-engine/docs/regions) or use the default '`us-central1`'. The region is where the model will be deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PROJECT_ID=mwe-sanofi-ml-workshop\n",
      "env: BUCKET_ID=ml-lending-club-demo\n",
      "env: REGION=us-central1\n",
      "env: TRAINER_PACKAGE_PATH=./lending_club_hp_tuning\n",
      "env: MAIN_TRAINER_MODULE=lending_club_hp_tuning.train\n",
      "env: RUNTIME_VERSION=1.15\n",
      "env: PYTHON_VERSION=3.7\n",
      "env: HPTUNING_CONFIG=hptuning_config.yaml\n"
     ]
    }
   ],
   "source": [
    "# Replace <PROJECT_ID> and <BUCKET_ID> with proper Project and Bucket ID's:\n",
    "%env PROJECT_ID mwe-sanofi-ml-workshop\n",
    "%env BUCKET_ID ml-lending-club-demo\n",
    "%env REGION us-central1\n",
    "%env TRAINER_PACKAGE_PATH ./lending_club_hp_tuning\n",
    "%env MAIN_TRAINER_MODULE lending_club_hp_tuning.train\n",
    "%env RUNTIME_VERSION 1.15\n",
    "%env PYTHON_VERSION 3.7\n",
    "%env HPTUNING_CONFIG hptuning_config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://ml-lending-club-demo/...\n",
      "ServiceException: 409 Bucket ml-lending-club-demo already exists.\n",
      "Copying file://Lending Club Data - DR_Demo_Lending_Club.tsv [Content-Type=text/tab-separated-values]...\n",
      "/ [1 files][  4.2 MiB/  4.2 MiB]                                                \n",
      "Operation completed over 1 objects/4.2 MiB.                                      \n"
     ]
    }
   ],
   "source": [
    "# Create bucket if it doesn't exist.\n",
    "!gsutil mb gs://${BUCKET_ID}\n",
    "\n",
    "# Training file must be in a cloud storage bucket before training runs.\n",
    "!gsutil cp 'Lending Club Data - DR_Demo_Lending_Club.tsv' gs://${BUCKET_ID}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "Path(\"./lending_club_hp_tuning/\").mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Create your python model file\n",
    "\n",
    "First, we'll create the python model file (provided below) that we'll upload to AI Platform. This is similar to your normal process for creating a XGBoost model. However, there are a few key differences:\n",
    "1. Downloading the data from GCS at the start of your file, so that AI Platform can access the data.\n",
    "1. Exporting/saving the model to GCS at the end of your file, so that you can use it for predictions.\n",
    "1. Define a command-line argument in your main training module for each tuned hyperparameter.\n",
    "1. Use the value passed in those arguments to set the corresponding hyperparameter in your application's XGBoost code.\n",
    "1. Use `cloudml-hypertune` to track your training jobs metrics.\n",
    "\n",
    "The code in this file first handles the hyperparameters passed to the file from AI Platform. Then it loads the data into a pandas DataFrame that can be used by XGBoost. Then the model is fit against the training data and the metrics for that data are shared with AI Platform. Lastly, Python's built in pickle library is used to save the model to a file that can be uploaded to [AI Platform's prediction service](https://cloud.google.com/ml-engine/docs/scikit/getting-predictions#deploy_models_and_versions).\n",
    "\n",
    "Note: In normal practice you would want to test your model locally on a small dataset to ensure that it works, before using it with your larger dataset on AI Platform. This avoids wasted time and costs.\n",
    "\n",
    "### Setup the imports and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./lending_club_hp_tuning/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./lending_club_hp_tuning/train.py\n",
    "\n",
    "import argparse\n",
    "import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import pickle\n",
    "\n",
    "from google.cloud import storage\n",
    "import hypertune\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the hyperparameter values that are passed to the model during training.\n",
    "\n",
    "In this tutorial, the Lasso regressor is used, because it has several parameters that can be used to help demonstrate how to choose HP tuning values. (The range of values are set below in the configuration file for the HP tuning values.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./lending_club_hp_tuning/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a ./lending_club_hp_tuning/train.py\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    '--job-dir',  # handled automatically by AI Platform\n",
    "    help='GCS location to write checkpoints and export models',\n",
    "    required=True\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--max_depth',  # Specified in the config file\n",
    "    help='Maximum depth of the XGBoost tree. default: 3',\n",
    "    default=3,\n",
    "    type=int\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--num_boost_round',  # Specified in the config file\n",
    "    help='Number of boosting iterations. default: 100',\n",
    "    default=100,\n",
    "    type=int\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--booster',  # Specified in the config file\n",
    "    help='which booster to use: gbtree, gblinear or dart. default: gbtree',\n",
    "    default='gbtree',\n",
    "    type=str\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--project-id',\n",
    "    type=str,\n",
    "    default='mwe-sanofi-ml-workshop',\n",
    "    help='The GCP Project ID'\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--bucket-name',\n",
    "    type=str,\n",
    "    default='ml-lending-club-demo',\n",
    "    help='The Cloud Storage bucket to be used for process artifacts'\n",
    ")\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add code to download the data from GCS\n",
    "In this case, using the publicly hosted data,AI Platform will then be able to use the data when training your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./lending_club_hp_tuning/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a ./lending_club_hp_tuning/train.py\n",
    "\n",
    "# Bucket holding the lending club data\n",
    "bucket = storage.Client().bucket(args.bucket_name)\n",
    "# Path to the data inside the bucket\n",
    "blob = bucket.blob('Lending Club Data - DR_Demo_Lending_Club.tsv')\n",
    "# Download the data\n",
    "blob.download_to_filename('Lending Club Data - DR_Demo_Lending_Club.tsv')\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Return first three digits of zip code.\n",
    "z3 = lambda z : int(z[:3])\n",
    "\n",
    "def dateparse(dt):\n",
    "    # test for empty string '' or NaT\n",
    "    if not dt or pd.isnull(dt):\n",
    "        return pd.NaT\n",
    "    m, d, y = map(int, dt.split('/'))\n",
    "    # 68 ==> 1968, 07 ==> 2007\n",
    "    y = y+1900 if y > 20 else y+2000\n",
    "    return pd.datetime(y, m, d)\n",
    "\n",
    "schema= pd.Series({\n",
    "    'Id': 'int64',\n",
    "    'is_bad': 'int64',\n",
    "    # 'emp_title': 'category',\n",
    "    'emp_length': 'float64',\n",
    "    'home_ownership': 'category',\n",
    "    'annual_inc': 'float64',\n",
    "    'verification_status': 'category',\n",
    "    # 'pymnt_plan': 'category',\n",
    "    # 'Notes': 'category',\n",
    "    'purpose_cat': 'category',\n",
    "    # 'purpose': 'category',\n",
    "    'zip_code': 'int64',\n",
    "    'addr_state': 'category',\n",
    "    'debt_to_income': 'float64',\n",
    "    'delinq_2yrs': 'float64',\n",
    "    'earliest_cr_line': 'datetime64[ns]',\n",
    "    'inq_last_6mths': 'float64',\n",
    "    'mths_since_last_delinq': 'float64',\n",
    "    'mths_since_last_record': 'float64',\n",
    "    'open_acc': 'float64',\n",
    "    'pub_rec': 'float64',\n",
    "    'revol_bal': 'float64',\n",
    "    'revol_util': 'float64',\n",
    "    'total_acc': 'float64',\n",
    "    # 'initial_list_status': 'category',\n",
    "    # 'collections_12_mths_ex_med': 'Int64',\n",
    "    'mths_since_last_major_derog': 'int64',\n",
    "    'policy_code': 'category'\n",
    "})\n",
    "\n",
    "usecols=schema.index\n",
    "# zip_code and earliest_cr_line will be parsed through converters.\n",
    "dtype=schema[~schema.index.isin(['zip_code', 'earliest_cr_line'])].to_dict()\n",
    "\n",
    "lcd = pd.read_csv('./Lending Club Data - DR_Demo_Lending_Club.tsv', \n",
    "    sep='\\t', \n",
    "    index_col='Id',\n",
    "    na_values={'emp_length': 'na'},\n",
    "    keep_default_na=True,\n",
    "    usecols=usecols,\n",
    "    dtype=dtype,\n",
    "    converters={'zip_code':z3, 'earliest_cr_line':dateparse}\n",
    ")\n",
    "\n",
    "lcd['emp_length'].fillna(1, inplace=True)\n",
    "lcd['emp_length'].clip(upper=10, inplace=True)\n",
    "lcd['emp_length']=lcd['emp_length'].astype('int64')\n",
    "# lcd['emp_length'].value_counts(dropna=False)\n",
    "\n",
    "\n",
    "# lcd['zip_code'].dtype\n",
    "# dtype('int64')\n",
    "\n",
    "lcd['home_ownership'].replace('NONE', 'OTHER', inplace=True)\n",
    "# lcd['home_ownership'].value_counts(dropna=False)\n",
    "\n",
    "# lcd['annual_inc'].median()\n",
    "lcd['annual_inc'].fillna(58000, inplace=True)\n",
    "lcd['annual_inc'].clip(upper=250000, inplace=True)\n",
    "# lcd['annual_inc'].value_counts(dropna=False)\n",
    "\n",
    "sb_flag=lcd['purpose_cat'].str[-14:]=='small business'\n",
    "lcd.loc[sb_flag, 'purpose_cat'] = 'small business'\n",
    "# np.where(sb_flag, 'small business', lcd['purpose_cat'])\n",
    "# lcd['purpose_cat'].value_counts(dropna=False)\n",
    "\n",
    "\n",
    "lcd['delinq_2yrs'].fillna(0, inplace=True)\n",
    "lcd['delinq_2yrs'].clip(upper=3, inplace=True)\n",
    "lcd['delinq_2yrs']=lcd['delinq_2yrs'].astype('int64')\n",
    "# lcd['delinq_2yrs'].value_counts(dropna=False)\n",
    "\n",
    "lcd['inq_last_6mths'].fillna(0, inplace=True)\n",
    "lcd['inq_last_6mths'].clip(upper=4, inplace=True)\n",
    "lcd['inq_last_6mths']=lcd['inq_last_6mths'].astype('int64')\n",
    "# lcd['inq_last_6mths'].value_counts(dropna=False)\n",
    "# lcd.groupby('inq_last_6mths', observed=True).agg({'is_bad': ['sum', 'count']})\n",
    "\n",
    "lcd['mths_since_last_delinq'].fillna(120.0, inplace=True)\n",
    "# lcd.groupby('mths_since_last_delinq', observed=True).agg({'is_bad': ['sum', 'count']})\n",
    "\n",
    "lcd['mths_since_last_record'].fillna(0.0, inplace=True)\n",
    "lcd['inq_last_6mths']=lcd['inq_last_6mths'].astype('int64')\n",
    "# lcd.groupby('mths_since_last_record', observed=True).agg({'is_bad': ['sum', 'count']})\n",
    "\n",
    "lcd['open_acc'].fillna(7, inplace=True)\n",
    "lcd['open_acc']=lcd['open_acc'].astype('int64')\n",
    "# lcd['open_acc'].value_counts(dropna=False)\n",
    "# lcd.groupby('open_acc', observed=True).agg({'is_bad': ['sum', 'count']})\n",
    "\n",
    "lcd['pub_rec'].fillna(0, inplace=True)\n",
    "lcd['pub_rec'].clip(upper=1, inplace=True)\n",
    "lcd['pub_rec']=lcd['pub_rec'].astype('int64')\n",
    "# lcd['pub_rec'].value_counts(dropna=False)\n",
    "# lcd.groupby('pub_rec', observed=True).agg({'is_bad': ['sum', 'count']})\n",
    "\n",
    "lcd['revol_bal'].clip(upper=100000, inplace=True)\n",
    "# lcd.groupby('revol_bal', observed=True).agg({'is_bad': ['sum', 'count']})\n",
    "\n",
    "lcd['revol_util'].fillna(0, inplace=True)\n",
    "# lcd.groupby('revol_util', observed=True).agg({'is_bad': ['sum', 'count']})\n",
    "\n",
    "lcd['total_acc'].fillna(1, inplace=True)\n",
    "lcd['total_acc']=lcd['total_acc'].astype('int64')\n",
    "# lcd['total_acc'].value_counts(dropna=False)\n",
    "# lcd.groupby('total_acc', observed=True).agg({'is_bad': ['sum', 'count']})\n",
    "\n",
    "# lcd.groupby('collections_12_mths_ex_med').agg({'is_bad': ['sum', 'count']})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./lending_club_hp_tuning/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a ./lending_club_hp_tuning/train.py\n",
    "target = 'is_bad'\n",
    "numerical_features = lcd.select_dtypes(include=['number']).columns\n",
    "numerical_features = numerical_features.drop([target])\n",
    "\n",
    "categorical_features =lcd.select_dtypes(include=['category']).columns\n",
    "predictors=numerical_features.union(categorical_features, sort=False)\n",
    "\n",
    "# exec(open(\"/mnt/c/Users/bjaco/Documents/projects_2020/mavenwave/python/lcd_1/lcd_read_input_1.py\").read())\n",
    "# ecl=pd.concat([lcd['earliest_cr_line'], lcd[target]], axis=1, copy=True)\n",
    "# ecl.set_index('earliest_cr_line', inplace=True)\n",
    "# ecl_q = ecl.groupby(by=pd.Grouper(freq='Q')).agg({target: ['sum', 'count']})\n",
    "# pd.set_option('display.max_rows', 100)\n",
    "# print(ecl_q[-100:])\n",
    "\n",
    "#                  is_bad\n",
    "#                     sum count\n",
    "# earliest_cr_line\n",
    "# 2006-09-30           10    87\n",
    "# 2006-12-31           16    82\n",
    "# 2007-03-31           12    64\n",
    "# 2007-06-30           11    53\n",
    "# 2007-09-30            7    39\n",
    "# 2007-12-31            3    24\n",
    "# 2008-03-31            1    21\n",
    "# 2008-06-30            2     9\n",
    "# 2008-09-30            0     3\n",
    "# 2008-12-31            0     1\n",
    "\n",
    "lcd.reset_index('Id', inplace=True)\n",
    "lcd.set_index('earliest_cr_line', inplace=True)\n",
    "start_train='2002-7-1'\n",
    "start_test='2006-7-1'\n",
    "end_test='2007-7-1'\n",
    "\n",
    "lcd_train = lcd.loc[start_train:start_test].copy()\n",
    "lcd_test = lcd.loc[start_test:end_test].copy()\n",
    "\n",
    "\n",
    "lcd_train.reset_index('earliest_cr_line', drop=True, inplace=True)\n",
    "lcd_test.reset_index('earliest_cr_line', drop=True, inplace=True)\n",
    "\n",
    "# lcd.set_index(['Id'], append=True, inplace=True)\n",
    "lcd_train.set_index(['Id'], inplace=True)\n",
    "lcd_test.set_index(['Id'], inplace=True)\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=42)\n",
    "\n",
    "X_train=lcd_train[predictors].copy()\n",
    "y_train=lcd_train[target].copy()\n",
    "# X_train.head()\n",
    "\n",
    "X_test=lcd_test[predictors].copy()\n",
    "y_test=lcd_test[target].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./lending_club_hp_tuning/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a ./lending_club_hp_tuning/train.py\n",
    "# Manually one-hot-encode categorical variables.\n",
    "# https://towardsdatascience.com/categorical-encoding-using-label-encoding-and-one-hot-encoder-911ef77fb5bd\n",
    "for cat_var in list(categorical_features.values):\n",
    "    cat_col=pd.DataFrame(X_train[cat_var], columns=[cat_var])\n",
    "    dum_df = pd.get_dummies(cat_col, columns=[cat_var], prefix=cat_var)\n",
    "    X_train.drop(cat_var, axis=1, inplace=True)\n",
    "    X_train=X_train.join(dum_df)\n",
    "# X_train.columns\n",
    "\n",
    "for cat_var in list(categorical_features.values):\n",
    "    cat_col=pd.DataFrame(X_test[cat_var], columns=[cat_var])\n",
    "    dum_df = pd.get_dummies(cat_col, columns=[cat_var], prefix=cat_var)\n",
    "    X_test.drop(cat_var, axis=1, inplace=True)\n",
    "    X_test=X_test.join(dum_df)\n",
    "# X_test.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the Hyperparameters\n",
    "Use the Hyperparameter values passed in those arguments to set the corresponding hyperparameters in your application's XGBoost code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./lending_club_hp_tuning/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a ./lending_club_hp_tuning/train.py\n",
    "\n",
    "# Create the classifier, here we will use an XGboost classifier to demonstrate the use of HP Tuning.\n",
    "# Here is where we set the variables used during HP Tuning from\n",
    "# the parameters passed into the python script\n",
    "classifier = xgb.XGBClassifier(max_depth=args.max_depth,\n",
    "                             num_boost_round=args.num_boost_round,\n",
    "                             booster=args.booster,\n",
    "                             eval_metric='auc'\n",
    "                            )\n",
    "\n",
    "# Transform the features and fit them to the classifier\n",
    "# classifier.fit(train_df[FEATURES], train_df[TARGET])\n",
    "classifier.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report the mean accuracy as hyperparameter tuning objective metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./lending_club_hp_tuning/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a ./lending_club_hp_tuning/train.py\n",
    "\n",
    "# Calculate the mean accuracy on the given test data and labels.\n",
    "score = classifier.score(X_test, y_test)\n",
    "\n",
    "# The default name of the metric is training/hptuning/metric. \n",
    "# We recommend that you assign a custom name. The only functional difference is that \n",
    "# if you use a custom name, you must set the hyperparameterMetricTag value in the \n",
    "# HyperparameterSpec object in your job request to match your chosen name.\n",
    "# https://cloud.google.com/ml-engine/reference/rest/v1/projects.jobs#HyperparameterSpec\n",
    "hpt = hypertune.HyperTune()\n",
    "hpt.report_hyperparameter_tuning_metric(\n",
    "    hyperparameter_metric_tag='my_metric_tag',\n",
    "    metric_value=score,\n",
    "    global_step=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export and save the model to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./lending_club_hp_tuning/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a ./lending_club_hp_tuning/train.py\n",
    "\n",
    "# Export the model to a file\n",
    "model_filename = 'model.pkl'\n",
    "with open(model_filename, \"wb\") as f:\n",
    "    pickle.dump(classifier, f)\n",
    "\n",
    "# Example: job_dir = 'gs://BUCKET_ID/xgboost_job_dir/1'\n",
    "job_dir =  args.job_dir.replace('gs://', '')  # Remove the 'gs://'\n",
    "# Get the Bucket Id\n",
    "bucket_id = job_dir.split('/')[0]\n",
    "# Get the path\n",
    "bucket_path = job_dir[len('{}/'.format(bucket_id)):]  # Example: 'xgboost_job_dir/1'\n",
    "# (bucket_id, _, bucket_path) = job_dir.partition('/')\n",
    "\n",
    "# Upload the model to GCS\n",
    "bucket = storage.Client().bucket(bucket_id)\n",
    "blob = bucket.blob('{}/{}'.format(\n",
    "    bucket_path,\n",
    "    model_filename))\n",
    "\n",
    "blob.upload_from_filename(model_filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Create Trainer Package with Hyperparameter Tuning\n",
    "Next we need to build the Trainer Package, which holds all your code and dependencies need to train your model on AI Platform. \n",
    "\n",
    "First, we create an empty `__init__.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./lending_club_hp_tuning/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./lending_club_hp_tuning/__init__.py\n",
    "\n",
    "#!/usr/bin/env python\n",
    "\n",
    "# Note that __init__.py can be an empty file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to set the hp tuning values used to train our model. Check [HyperparameterSpec](https://cloud.google.com/ml-engine/reference/rest/v1/projects.jobs#HyperparameterSpec) for more info. \n",
    "\n",
    "In this config file several key things are set:\n",
    "* `maxTrials` - How many training trials should be attempted to optimize the specified hyperparameters.\n",
    "* `maxParallelTrials: 5` - The number of training trials to run concurrently. \n",
    "* `params` - The set of parameters to tune. These are the different parameters to pass into your model and the specified ranges you wish to try.\n",
    " * `parameterName` - The parameter name must be unique amongst all ParameterConfigs\n",
    " * `type` - The type of the parameter. [INTEGER, DOUBLE, ...]\n",
    " * `minValue` & `maxValue` - The range of values that this parameter could be. \n",
    " * `scaleType` - How the parameter should be scaled to the hypercube. Leave unset for categorical parameters. Some kind of scaling is strongly recommended for real or integral parameters (e.g., UNIT_LINEAR_SCALE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./hptuning_config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./hptuning_config.yaml\n",
    "\n",
    "#!/usr/bin/env python\n",
    "\n",
    "# Copyright 2018 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "# hyperparam.yaml\n",
    "trainingInput:\n",
    "  hyperparameters:\n",
    "    goal: MAXIMIZE\n",
    "    maxTrials: 5\n",
    "    maxParallelTrials: 5\n",
    "    hyperparameterMetricTag: my_metric_tag\n",
    "    enableTrialEarlyStopping: TRUE \n",
    "    params:\n",
    "    - parameterName: max_depth\n",
    "      type: INTEGER\n",
    "      minValue: 3\n",
    "      maxValue: 8\n",
    "    - parameterName: num_boost_round\n",
    "      type: INTEGER\n",
    "      minValue: 50\n",
    "      maxValue: 200\n",
    "    - parameterName: booster\n",
    "      type: CATEGORICAL\n",
    "      categoricalValues: [\n",
    "          \"gbtree\",\n",
    "          \"gblinear\",\n",
    "          \"dart\"\n",
    "      ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we need to install the dependencies used in our model. Check [adding_standard_pypi_dependencies](https://cloud.google.com/ml-engine/docs/tensorflow/packaging-trainer#adding_standard_pypi_dependencies) for more info.\n",
    "\n",
    "To do this, AI Platform uses a setup.py file to install your dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./setup.py\n",
    "\n",
    "#!/usr/bin/env python\n",
    "\n",
    "# Copyright 2018 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "from setuptools import find_packages\n",
    "from setuptools import setup\n",
    "\n",
    "REQUIRED_PACKAGES = ['cloudml-hypertune']\n",
    "\n",
    "setup(\n",
    "    name='lending_club_hp_tuning',\n",
    "    version='0.1',\n",
    "    install_requires=REQUIRED_PACKAGES,\n",
    "    packages=find_packages(),\n",
    "    include_package_data=True,\n",
    "    description='Lending Club HP tuning training application'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Submit Training Job\n",
    "Next we need to submit the job for training on AI Platform. We'll use gcloud to submit the job which has the following flags:\n",
    "\n",
    "* `job-name` - A name to use for the job (mixed-case letters, numbers, and underscores only, starting with a letter). In this case: `auto_mpg_hp_tuning_$(date +\"%Y%m%d_%H%M%S\")`\n",
    "* `job-dir` - The path to a Google Cloud Storage location to use for job output.\n",
    "* `package-path` - A packaged training application that is staged in a Google Cloud Storage location. If you are using the gcloud command-line tool, this step is largely automated.\n",
    "* `module-name` - The name of the main module in your trainer package. The main module is the Python file you call to start the application. If you use the gcloud command to submit your job, specify the main module name in the --module-name argument. Refer to Python Packages to figure out the module name.\n",
    "* `region` - The Google Cloud Compute region where you want your job to run. You should run your training job in the same region as the Cloud Storage bucket that stores your training data. Select a region from [here](https://cloud.google.com/ml-engine/docs/regions) or use the default '`us-central1`'.\n",
    "* `runtime-version` - The version of AI Platform to use for the job. If you don't specify a runtime version, the training service uses the default AI Platform runtime version 1.0. See the list of runtime versions for more information.\n",
    "* `python-version` - The Python version to use for the job. Python 3.5 is available with runtime version 1.4 or greater. If you don't specify a Python version, the training service uses Python 2.7.\n",
    "* `scale-tier` - A scale tier specifying the type of processing cluster to run your job on. This can be the CUSTOM scale tier, in which case you also explicitly specify the number and type of machines to use.\n",
    "* `config` - Path to the job configuration file. This file should be a YAML document (JSON also accepted) containing a Job resource as defined in the API\n",
    "\n",
    "Note: Check to make sure gcloud is set to the current PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: JOB_NAME=black_friday_job_20200614_204341\n",
      "Job [black_friday_job_20200614_204341] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe black_friday_job_20200614_204341\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs black_friday_job_20200614_204341\n",
      "jobId: black_friday_job_20200614_204341\n",
      "state: QUEUED\n",
      "SUCCEEDED\n"
     ]
    }
   ],
   "source": [
    "now = !date +\"%Y%m%d_%H%M%S\"\n",
    "%env JOB_NAME=black_friday_job_$now.s\n",
    "\n",
    "!gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "  --job-dir gs://${BUCKET_ID}/data \\\n",
    "  --package-path $TRAINER_PACKAGE_PATH \\\n",
    "  --module-name $MAIN_TRAINER_MODULE \\\n",
    "  --region $REGION \\\n",
    "  --runtime-version=$RUNTIME_VERSION \\\n",
    "  --python-version=$PYTHON_VERSION \\\n",
    "  --scale-tier basic \\\n",
    "  --config $HPTUNING_CONFIG \\\n",
    "  -- \\\n",
    "  --project-id $PROJECT_ID \\\n",
    "  --bucket-name ${BUCKET_ID}    \n",
    "    \n",
    "# Stream logs so that training is done before subsequent cells are run.\n",
    "# Remove  '> /dev/null' to see step-by-step output of the model build steps.\n",
    "!gcloud ai-platform jobs stream-logs $JOB_NAME > /dev/null\n",
    "\n",
    "# Model should exit with status \"SUCCEEDED\"\n",
    "!gcloud ai-platform jobs describe $JOB_NAME --format=\"value(state)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Optional] StackDriver Logging\n",
    "You can view the logs for your training job:\n",
    "1. Go to https://console.cloud.google.com/\n",
    "1. Select \"Logging\" in left-hand pane\n",
    "1. In left-hand pane, go to \"AI Platform\" and select Jobs\n",
    "1. In filter by prefix, use the value of $JOB_NAME to view the logs\n",
    "\n",
    "On the logging page of your model, you can view the different results for each HP tuning job. \n",
    "\n",
    "Example:\n",
    "```\n",
    "{\n",
    "  \"trialId\": \"15\",\n",
    "  \"hyperparameters\": {\n",
    "    \"booster\": \"dart\",\n",
    "    \"max_depth\": \"7\",\n",
    "    \"n_estimators\": \"102\"\n",
    "  },\n",
    "  \"finalMetric\": {\n",
    "    \"trainingStep\": \"1000\",\n",
    "    \"objectiveValue\": 0.9259230441279733\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Optional] Verify Model File in GCS\n",
    "View the contents of the destination model folder to verify that all 30 model files have indeed been uploaded to GCS.\n",
    "\n",
    "Note: The model can take a few minutes to train and show up in GCS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of the log output at the end of hyperparameter training showing the final AUC score and parameters chosen to achieve it:\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    finalMetric:\n",
    "      objectiveValue: 0.828859\n",
    "      trainingStep: '1000'\n",
    "    hyperparameters:\n",
    "      booster: gblinear\n",
    "      max_depth: '3'\n",
    "      num_boost_round: '112'\n",
    "    startTime: '2020-04-06T16:34:25.152556682Z'\n",
    "    state: SUCCEEDED\n",
    "    trialId: '12'"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf-gpu.1-15.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf-gpu.1-15:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
