{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the template file for creating the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./pipeline/sensor_training_pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./pipeline/sensor_training_pipeline.py\n",
    "import os\n",
    "import datetime\n",
    "from func_components import load_raw_data\n",
    "from func_components import split_data\n",
    "from func_components import disp_loss\n",
    "from jinja2 import Template\n",
    "import kfp\n",
    "from kfp.components import func_to_container_op\n",
    "from kfp.dsl.types import Dict\n",
    "from kfp.dsl.types import GCPProjectID\n",
    "from kfp.dsl.types import GCPRegion\n",
    "from kfp.dsl.types import GCSPath\n",
    "from kfp.dsl.types import String\n",
    "from kfp.gcp import use_gcp_secret\n",
    "\n",
    "# Defaults and environment settings\n",
    "BASE_IMAGE = os.getenv('BASE_IMAGE')\n",
    "TRAINER_IMAGE = os.getenv('TRAINER_IMAGE')\n",
    "DD_IMAGE = os.getenv('DD_IMAGE')\n",
    "RUNTIME_VERSION = os.getenv('RUNTIME_VERSION')\n",
    "PYTHON_VERSION = os.getenv('PYTHON_VERSION')\n",
    "COMPONENT_URL_SEARCH_PREFIX = os.getenv('COMPONENT_URL_SEARCH_PREFIX')\n",
    "USE_KFP_SA = os.getenv('USE_KFP_SA')\n",
    "\n",
    "# Create component factories\n",
    "component_store = kfp.components.ComponentStore(\n",
    "    local_search_paths=None, url_search_prefixes=[COMPONENT_URL_SEARCH_PREFIX])\n",
    "\n",
    "# Create all the component ops\n",
    "caip_train_op = component_store.load_component('ml_engine/train')\n",
    "\n",
    "retrieve_raw_data_op = func_to_container_op(\n",
    "    load_raw_data, base_image=BASE_IMAGE)\n",
    "\n",
    "split_preprocess_data_op = func_to_container_op(\n",
    "    split_data, base_image=BASE_IMAGE)\n",
    "\n",
    "disp_loss_op = func_to_container_op(\n",
    "    disp_loss)\n",
    "\n",
    "def datadescribe_op(gcs_root, filepath):\n",
    "    return kfp.dsl.ContainerOp(\n",
    "        name='Run_Data_Describe',\n",
    "        image = DD_IMAGE,\n",
    "        arguments=[\n",
    "            '--gcs_root', gcs_root,\n",
    "            '--file', filepath\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "# Define the pipeline\n",
    "@kfp.dsl.pipeline(\n",
    "    name='Bearing Sensor Data Training',\n",
    "    description='The pipeline for training and deploying an anomaly detector based on an autoencoder')\n",
    "\n",
    "def pipeline_run(project_id,\n",
    "                 region,\n",
    "                 source_bucket_name, \n",
    "                 prefix,\n",
    "                 dest_bucket_name,\n",
    "                 dest_file_name,\n",
    "                 gcs_root,\n",
    "                 dataset_location='US'):\n",
    "    \n",
    "    # Read in the raw sensor data from the public dataset and load in the project bucket\n",
    "    raw_data = retrieve_raw_data_op(source_bucket_name,\n",
    "                                    prefix,\n",
    "                                    dest_bucket_name,\n",
    "                                    dest_file_name)\n",
    "    \n",
    "    \n",
    "    # Prepare some output from Data Describe\n",
    "    dd_out = datadescribe_op(gcs_root, \n",
    "                             raw_data.outputs['dest_file_name'])\n",
    "    \n",
    "    \n",
    "    # Preprocess and split the raw data by time\n",
    "    split_data = split_preprocess_data_op(raw_data.outputs['dest_bucket_name'],\n",
    "                                          raw_data.outputs['dest_file_name'],\n",
    "                                          '2004-02-15 12:52:39',\n",
    "                                          True)\n",
    "    \n",
    "    # Set up the training args\n",
    "    train_args = [\"--bucket\", split_data.outputs['bucket_name'],\n",
    "                  \"--train_file\", split_data.outputs['train_dest_file'],\n",
    "                  \"--test_file\", split_data.outputs['test_dest_file']\n",
    "                 ]\n",
    "    \n",
    "    job_dir = \"{0}/{1}/{2}\".format(gcs_root, 'jobdir', kfp.dsl.RUN_ID_PLACEHOLDER)\n",
    "    \n",
    "    # Train the model on AI Platform\n",
    "    train_model = caip_train_op(project_id,\n",
    "                                region=region,\n",
    "                                master_image_uri=TRAINER_IMAGE,\n",
    "                                python_version=\"3.7\",\n",
    "                                job_id_prefix=f'anomaly-detection-{datetime.datetime.now().strftime(\"%H%M%S\")}_',\n",
    "                                job_dir=job_dir,\n",
    "                                args=train_args)\n",
    "    \n",
    "    # Expose artifacts to the Kubeflow UI\n",
    "    disp_loss_img = disp_loss_op(train_model.outputs['job_id'])\n",
    "    disp_loss_dist_img = disp_loss_op(train_model.outputs['job_id'])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "ENDPOINT = 'https://fda9da3634d2db2-dot-us-central2.pipelines.googleusercontent.com'\n",
    "BUCKET_NAME = 'dlaw_bucket'\n",
    "ARTIFACT_STORE_URI = f'gs://{BUCKET_NAME}'\n",
    "PROJECT_ID = \"mwpmltr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: ENDPOINT=https://fda9da3634d2db2-dot-us-central2.pipelines.googleusercontent.com\n",
      "env: PROJECT_ID=ups-ai-ml-sandbox\n",
      "env: REGION=us-central1\n",
      "env: BUCKET_NAME=dlaw_bucket\n",
      "env: ARTIFACT_STORE_URI=gs://dlaw_bucket\n"
     ]
    }
   ],
   "source": [
    "%env ENDPOINT=$ENDPOINT\n",
    "%env PROJECT_ID=$PROJECT_ID\n",
    "%env REGION=$REGION\n",
    "%env BUCKET_NAME=$BUCKET_NAME\n",
    "%env ARTIFACT_STORE_URI=$ARTIFACT_STORE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the base image and load it into gcr.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME='nasa-iot-base'\n",
    "TAG='v1'\n",
    "BASE_IMAGE='gcr.io/{}/{}:{}'.format(PROJECT_ID, IMAGE_NAME, TAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DON'T RUN THIS IF THE IMAGE EXISTS!\n",
    "# !gcloud builds submit --timeout 15m --tag $BASE_IMAGE base_image --async"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the training image from the base image and load it into the gcr.io (maybe just have one image?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME='nasa-iot-trainer'\n",
    "TAG='v5'\n",
    "TRAINER_IMAGE='gcr.io/{}/{}:{}'.format(PROJECT_ID, IMAGE_NAME, TAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DON'T RUN THIS IF THE IMAGE EXISTS!\n",
    "# !gcloud builds submit --timeout 15m --tag $TRAINER_IMAGE train_image --async"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Data Describe image from the base image and load it into gcr.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NAME='nasa-iot-datadescribe'\n",
    "TAG='v1'\n",
    "DD_IMAGE='gcr.io/{}/{}:{}'.format(PROJECT_ID, IMAGE_NAME, TAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DON'T RUN THIS IF THE IMAGE EXISTS!\n",
    "# !gcloud builds submit --timeout 15m --tag $DD_IMAGE dd_image --async"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: USE_KFP_SA=False\n",
      "env: BASE_IMAGE=gcr.io/ups-ai-ml-sandbox/nasa-iot-base:v1\n",
      "env: TRAINER_IMAGE=gcr.io/ups-ai-ml-sandbox/nasa-iot-trainer:v5\n",
      "env: ENDPOINT=https://fda9da3634d2db2-dot-us-central2.pipelines.googleusercontent.com\n",
      "env: DD_IMAGE=gcr.io/ups-ai-ml-sandbox/nasa-iot-datadescribe:v1\n",
      "env: COMPONENT_URL_SEARCH_PREFIX=https://raw.githubusercontent.com/kubeflow/pipelines/0.2.5/components/gcp/\n",
      "env: RUNTIME_VERSION=1.15\n",
      "env: PYTHON_VERSION=3.7\n"
     ]
    }
   ],
   "source": [
    "USE_KFP_SA = False\n",
    "\n",
    "COMPONENT_URL_SEARCH_PREFIX = 'https://raw.githubusercontent.com/kubeflow/pipelines/0.2.5/components/gcp/'\n",
    "RUNTIME_VERSION = '1.15'\n",
    "PYTHON_VERSION = '3.7'\n",
    "\n",
    "%env USE_KFP_SA={USE_KFP_SA}\n",
    "%env BASE_IMAGE={BASE_IMAGE}\n",
    "%env TRAINER_IMAGE={TRAINER_IMAGE}\n",
    "%env ENDPOINT={ENDPOINT}\n",
    "%env DD_IMAGE={DD_IMAGE}\n",
    "%env COMPONENT_URL_SEARCH_PREFIX={COMPONENT_URL_SEARCH_PREFIX}\n",
    "%env RUNTIME_VERSION={RUNTIME_VERSION}\n",
    "%env PYTHON_VERSION={PYTHON_VERSION}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dsl-compile --py pipeline/sensor_training_pipeline.py --output sensor_training_pipeline.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List the Pipeline in AI Platform Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline 17e93454-fb5b-40ba-8d17-39322c3f1d3b has been submitted\n",
      "\n",
      "Pipeline Details\n",
      "------------------\n",
      "ID           17e93454-fb5b-40ba-8d17-39322c3f1d3b\n",
      "Name         bearing_sensor_anomaly_v1.0\n",
      "Description\n",
      "Uploaded at  2020-12-09T03:08:11+00:00\n",
      "+--------------------+-----------------+\n",
      "| Parameter Name     | Default Value   |\n",
      "+====================+=================+\n",
      "| project_id         |                 |\n",
      "+--------------------+-----------------+\n",
      "| region             |                 |\n",
      "+--------------------+-----------------+\n",
      "| source_bucket_name |                 |\n",
      "+--------------------+-----------------+\n",
      "| prefix             |                 |\n",
      "+--------------------+-----------------+\n",
      "| dest_bucket_name   |                 |\n",
      "+--------------------+-----------------+\n",
      "| dest_file_name     |                 |\n",
      "+--------------------+-----------------+\n",
      "| gcs_root           |                 |\n",
      "+--------------------+-----------------+\n",
      "| dataset_location   | US              |\n",
      "+--------------------+-----------------+\n"
     ]
    }
   ],
   "source": [
    "PIPELINE_NAME='bearing_sensor_anomaly_v1.0'\n",
    "\n",
    "!kfp --endpoint $ENDPOINT pipeline upload \\\n",
    "-p $PIPELINE_NAME \\\n",
    "sensor_training_pipeline.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n",
      "| Pipeline ID                          | Name                                            | Uploaded at               |\n",
      "+======================================+=================================================+===========================+\n",
      "| 17e93454-fb5b-40ba-8d17-39322c3f1d3b | bearing_sensor_anomaly_v1.0                     | 2020-12-09T03:08:11+00:00 |\n",
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n",
      "| 4a62afc3-e9c3-450d-8500-1ea3454703fb | [Tutorial] DSL - Control structures             | 2020-12-09T01:06:29+00:00 |\n",
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n",
      "| da69e0c9-35e0-4ea5-a80c-223ab7dee137 | [Tutorial] Data passing in python components    | 2020-12-09T01:06:28+00:00 |\n",
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n",
      "| 9873fae7-3ecc-4299-90a3-769f633f075f | [Demo] TFX - Iris classification pipeline       | 2020-12-09T01:06:27+00:00 |\n",
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n",
      "| 6459fdff-d667-4a4d-b13e-b1b8e5c16f79 | [Demo] TFX - Taxi tip prediction model trainer  | 2020-12-09T01:06:26+00:00 |\n",
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n",
      "| 5304ed66-0dd5-4e22-9c0c-9fee97ba5185 | [Demo] XGBoost - Training with confusion matrix | 2020-12-09T01:06:25+00:00 |\n",
      "+--------------------------------------+-------------------------------------------------+---------------------------+\n"
     ]
    }
   ],
   "source": [
    "!kfp --endpoint $ENDPOINT pipeline list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit a Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIPELINE_ID='17e93454-fb5b-40ba-8d17-39322c3f1d3b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = 'AnomalyDetector'\n",
    "RUN_ID = 'Run_001'\n",
    "SOURCE_BUCKET_NAME = 'amazing-public-data'\n",
    "PREFIX = 'bearing_sensor_data/bearing_sensor_data/'\n",
    "DEST_BUCKET_NAME = BUCKET_NAME\n",
    "DEST_FILE_NAME = 'raw_bearing_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PIPELINE_ID=17e93454-fb5b-40ba-8d17-39322c3f1d3b\n",
      "env: EXPERIMENT_NAME=AnomalyDetector\n",
      "env: RUN_ID=Run_001\n",
      "env: SOURCE_BUCKET_NAME=amazing-public-data\n",
      "env: PREFIX=bearing_sensor_data/bearing_sensor_data/\n",
      "env: DEST_BUCKET_NAME=dlaw_bucket\n",
      "env: DEST_FILE_NAME=raw_bearing_data.csv\n"
     ]
    }
   ],
   "source": [
    "%env PIPELINE_ID=$PIPELINE_ID\n",
    "%env EXPERIMENT_NAME=$EXPERIMENT_NAME\n",
    "%env RUN_ID=$RUN_ID\n",
    "%env SOURCE_BUCKET_NAME=$SOURCE_BUCKET_NAME\n",
    "%env PREFIX=$PREFIX\n",
    "%env DEST_BUCKET_NAME=$DEST_BUCKET_NAME\n",
    "%env DEST_FILE_NAME=$DEST_FILE_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 8eebcc03-cb9d-41d2-b35e-c6242fffd153 is submitted\n",
      "+--------------------------------------+---------+----------+---------------------------+\n",
      "| run id                               | name    | status   | created at                |\n",
      "+======================================+=========+==========+===========================+\n",
      "| 8eebcc03-cb9d-41d2-b35e-c6242fffd153 | Run_001 |          | 2020-12-09T03:10:34+00:00 |\n",
      "+--------------------------------------+---------+----------+---------------------------+\n"
     ]
    }
   ],
   "source": [
    "!kfp --endpoint $ENDPOINT run submit \\\n",
    "-e $EXPERIMENT_NAME \\\n",
    "-r $RUN_ID \\\n",
    "-p $PIPELINE_ID \\\n",
    "project_id=$PROJECT_ID \\\n",
    "gcs_root=$ARTIFACT_STORE_URI \\\n",
    "region=$REGION \\\n",
    "source_bucket_name=$SOURCE_BUCKET_NAME \\\n",
    "prefix=$PREFIX \\\n",
    "dest_bucket_name=$DEST_BUCKET_NAME \\\n",
    "dest_file_name=$DEST_FILE_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf-gpu.1-15.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf-gpu.1-15:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
